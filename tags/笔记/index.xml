<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>笔记 on 卢肇的博客</title>
    <link>/tags/%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 笔记 on 卢肇的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright> [Farewell](https://www.ifarewell.xyz/) All rights reserved.</copyright>
    <lastBuildDate>Tue, 26 Oct 2021 16:00:13 +0800</lastBuildDate><atom:link href="/tags/%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Federated Deep Learning with Bayesian Privacy 阅读笔记</title>
      <link>/posts/federated_deep_learning_with_bayesian_privacy/</link>
      <pubDate>Tue, 26 Oct 2021 16:00:13 +0800</pubDate>
      
      <guid>/posts/federated_deep_learning_with_bayesian_privacy/</guid>
      <description>前段时间 刷讲座课学分 有幸参加了微众银行（WeBank）的范力欣博士与康焱博士的讲座“联邦学习：目的、方法及应用”，对其中提到的Passport方法感到好奇，因此对相关论文进行了阅读，并在组会上进行了分享。在此对组会上的PPT内容进行整理记录。
背景 联邦学习是一种在保护数据隐私的前提下，通过各方合作训练深度学习模型来充分发挥各方数据的价值的方法。因其在安全且充分利用各方数据资源方面的作用而受到广泛关注。
但是，面对包含大量参数的深度学习模型，当前联邦学习中运用的隐私保护方案存在各种问题。
 模型准确率低 隐私保护性不高 模型收敛速度慢，计算、通信开销大  这三个方向的问题，实际上构成了联邦学习中的一组不可能三角，即单一隐私保护方案难以在三方面都有完美表现。
  在这种背景下，作者希望寻求一种能够在三方面都有不错表现的隐私保护方案。
相关工作 本文从三个方面对相关工作进行了总结归纳，包括隐私定义、隐私保护机制与隐私泄露攻击，其中对前两个方面的总结归纳比较到位。
隐私定义   $(\epsilon,\delta)-$差分隐私(Differential Privacy):
$\qquad$一个算法被认为满足$(\epsilon,\delta)-$差分隐私，当且仅当对于两个相距为1的数据集$S,S^\prime$，对于任意的输出集合$E$，应该有
$$\mathbb{P}(M(S)\in E)\leq e^\epsilon\mathbb{P}(M(S^\prime)\in E)+\delta.$$
这种隐私性定义方式提出较久，且较为严密，被较为广泛地接受,有较多基于该方式的后续工作。
  贝叶斯隐私(Baysian Privacy):
$\qquad$贝叶斯隐私计算攻击者对数据的先验分布与其通过暴露的公开信息恢复出的数据后验分布之间的KL散度(Kullback-Leibler divergence)，分析两分布之间的差异，以衡量隐私数据的泄露程度。
  隐私保护机制   基于随机加噪的保护机制
$\qquad$通过随机噪声的引入对交换的数据进行扰动，以达到保护隐私数据的目的。运算较为简单，受到较大关注，但不可避免地带来模型训练收敛速度较慢、最终模型准确率不高以及隐私保护能力不强等问题。
  基于同态加密的保护机制
$\qquad$通过算法设计，使得基于密文的计算在解密后的结果与直接基于明文的计算结果相同。能够很好地保证安全性，但计算和通信开销较高。
    基于拆分学习的保护机制
$\qquad$将模型拆分为公有部分和私有部分。中心服务器只对公有模型部分进行参数的聚合与分发，通过对于私有模型的隐藏来保证安全性。但实验表明，进攻者仍然可以通过参与方暴露给中心服务器的数据来推断出私有模型数据。
    基于Passport层的保护机制
$\qquad$对一个卷积层嵌入passport的方法如图。正常的卷积操作只有左边部分。而嵌入passport后，事先设定的参数$p_γ^l、p_β^l$同样经过卷积层，再经过编码器、解码器池化层之后分别得到参数γ、β用于对正常输出进行伸缩和偏移变换。
  $\qquad$该方法实际上不是作为一个隐私保护方案被提出的，而是为了调节模型输出，以达到验证模型所有权及知识产权的目的。根据前作的解释，passport层的引入是为了给伸缩参数γ、偏移参数β与网络的权重之间引入相关性，使得当输入非法passport时，模型的结果将产生较大偏差。这里参数$p_γ^l、p_β^l$的作用类似于密码学中的私钥。
  贝叶斯隐私 本文基于贝叶斯隐私框架对隐私保护与攻击两类对抗策略进行了较为深入的形式化分析，这也是本文在写作上比较值得借鉴的地方。
问题假定  多组织通过横向联邦学习共同进行模型训练DNN模型。 某些参与方（或中央服务器）是半诚实的，即不会提交错误数据或恶意执行错误计算，但可能利用联邦学习过程中交换的信息对其他参与方进行隐私攻击。 单个参与方拥有隐私数据 $x$，并会暴露公共数据 $G$，此处 $G$ 与 $x$ 相关，满足映射 $G=g(x)$。需要注意的是，在联邦学习中，映射 $g()$ 被所有参与方知晓，因为其训练的模型结构是相同的。但隐私数据 $x$ 仅被参与方自身了解。 分类任务中 G的计算通过最小化损失函数来完成。 半诚实参与方会试图通过最小化 $|g(\widetilde x)-G|$，以达到用 $\widetilde x$ 来估计 $x$ 的目的。  在上述问题假定下，存在两方相互对抗的机制。即隐私数据保护机制 $M$ 与隐私攻击机制 $A$。</description>
    </item>
    
  </channel>
</rss>
